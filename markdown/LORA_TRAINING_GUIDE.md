# LoRA å¾®è°ƒæ–¹æ¡ˆï¼šè®­ç»ƒä¸“å±åŠ å¯†è´§å¸äº¤æ˜“æ¨¡å‹

> æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•è®­ç»ƒä¸€ä¸ªé€‚ç”¨äºåŠ å¯†è´§å¸è‡ªåŠ¨äº¤æ˜“ç³»ç»Ÿçš„ AI æ¨¡å‹ã€‚

## ç›®å½•

1. [æ–¹æ¡ˆæ¦‚è¿°](#æ–¹æ¡ˆæ¦‚è¿°)
2. [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
3. [å‡†å¤‡è®­ç»ƒæ•°æ®](#å‡†å¤‡è®­ç»ƒæ•°æ®)
4. [æ•°æ®æ ¼å¼åŒ–](#æ•°æ®æ ¼å¼åŒ–)
5. [é…ç½®å¹¶å¯åŠ¨ LoRA è®­ç»ƒ](#é…ç½®å¹¶å¯åŠ¨-lora-è®­ç»ƒ)
6. [æ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°](#æ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°)
7. [é›†æˆåˆ°äº¤æ˜“ç³»ç»Ÿ](#é›†æˆåˆ°äº¤æ˜“ç³»ç»Ÿ)
8. [æŒç»­ä¼˜åŒ–](#æŒç»­ä¼˜åŒ–)
9. [æˆæœ¬ä¸æ—¶é—´ä¼°ç®—](#æˆæœ¬ä¸æ—¶é—´ä¼°ç®—)

---

## æ–¹æ¡ˆæ¦‚è¿°

### ä¸ºä»€ä¹ˆé€‰æ‹© LoRAï¼Ÿ

LoRAï¼ˆLow-Rank Adaptationï¼‰æ˜¯ä¸€ç§å‚æ•°é«˜æ•ˆçš„å¾®è°ƒæŠ€æœ¯ï¼š

| ç‰¹æ€§ | å…¨é‡å¾®è°ƒ | LoRA å¾®è°ƒ |
|------|----------|-----------|
| å‚æ•°é‡ | 100% | 1-5% |
| æ˜¾å­˜å ç”¨ | é«˜ | ä½ï¼ˆå¯ä½è‡³ 8GBï¼‰ |
| è®­ç»ƒé€Ÿåº¦ | æ…¢ | å¿« |
| ä¿ç•™åŸæœ‰èƒ½åŠ› | å¯èƒ½é—å¿˜ | ä¿æŒ |
| å®ç°å¤æ‚åº¦ | é«˜ | ä½ |

### æ¨èæ¨¡å‹

| æ¨¡å‹ | å‚æ•°é‡ | æ¨ç†æ˜¾å­˜ | å¾®è°ƒæ˜¾å­˜ | ç‰¹ç‚¹ |
|------|--------|----------|----------|------|
| **Qwen2.5-7B-Instruct** | 7B | 8GB | 16GB | ä¸­æ–‡å‹å¥½ï¼Œæ•ˆæœå¥½ |
| **Llama-3-8B-Instruct** | 8B | 8GB | 16GB | ç¤¾åŒºæ´»è·ƒï¼Œè‹±æ–‡ä¸ºä¸» |
| **Mistral-7B-Instruct** | 7B | 8GB | 16GB | æ¨ç†é€Ÿåº¦å¿« |
| **Gemma-2-9B-It** | 9B | 10GB | 20GB | Google ç”Ÿæ€ |

### æŠ€æœ¯æ ˆ

- **PEFT**ï¼šHugging Face å‚æ•°é«˜æ•ˆå¾®è°ƒåº“
- **Unsloth**ï¼šåŠ é€Ÿè®­ç»ƒï¼Œæ˜¾å­˜å‡åŠ
- **Transformers**ï¼šæ¨¡å‹åŠ è½½å’Œæ¨ç†
- **BitsAndBytes**ï¼š8bit/4bit é‡åŒ–

---

## ç¯å¢ƒå‡†å¤‡

### 1.1 åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# ä½¿ç”¨ Conda
conda create -n trading-llm python=3.10 -y
conda activate trading-llm

# æˆ–ä½¿ç”¨ venv
python -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate  # Windows
```

### 1.2 å®‰è£…ä¾èµ–

```bash
# å®‰è£… PyTorchï¼ˆCUDA 11.8ï¼‰
pip install torch==2.1.0 torchvision torchaudio \
    --index-url https://download.pytorch.org/whl/cu118

# å®‰è£…åŸºç¡€ä¾èµ–
pip install transformers>=4.40.0
pip install accelerate>=0.25.0

# å®‰è£… LoRA ç›¸å…³
pip install peft>=0.10.0
pip install bitsandbytes>=0.42.0

# å®‰è£… Unslothï¼ˆå¼ºçƒˆæ¨èï¼‰
pip install unsloth[colab] @ https://github.com/unslothai/unsloth/releases/download/v0.3.4/unsloth-0.3.4-py3-none-any.whl

# å®‰è£…æ•°æ®å¤„ç†
pip install datasets>=2.14.0
pip install pandas numpy scikit-learn

# éªŒè¯å®‰è£…
python -c "import torch; print(f'PyTorch: {torch.__version__}')"
python -c "import transformers; print(f'Transformers: {transformers.__version__}')"
python -c "import peft; print(f'PEFT: {peft.__version__}')"
python -c "import unsloth; print(f'Unsloth: {unsloth.__version__}')"
```

### 1.3 ç¡¬ä»¶è¦æ±‚

```bash
# æ£€æŸ¥ GPU
nvidia-smi

# æ¨èé…ç½®
# - GPU: RTX 4080 16GB æˆ– RTX 4090 24GB
# - å†…å­˜: 32GB RAM
# - å­˜å‚¨: 50GB SSD

# éªŒè¯ CUDA
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU count: {torch.cuda.device_count()}')"
python -c "import torch; print(f'GPU name: {torch.cuda.get_device_name(0)}')"
```

### 1.4 é¡¹ç›®ç»“æ„

```
alpha-trading-bot-okx/
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ training/
â”‚       â”œâ”€â”€ prepare_data.py        # æ•°æ®æ”¶é›†
â”‚       â”œâ”€â”€ convert_format.py      # æ ¼å¼è½¬æ¢
â”‚       â”œâ”€â”€ train_lora.py         # è®­ç»ƒè„šæœ¬
â”‚       â”œâ”€â”€ test_model.py          # æµ‹è¯•è„šæœ¬
â”‚       â””â”€â”€ evaluate.py            # è¯„ä¼°è„šæœ¬
â”œâ”€â”€ trading-llm-lora/             # è®­ç»ƒè¾“å‡ºç›®å½•
â”‚   â”œâ”€â”€ final_lora/               # æœ€ç»ˆæ¨¡å‹
â”‚   â””â”€â”€ logs/                     # è®­ç»ƒæ—¥å¿—
â””â”€â”€ training_data/
    â”œâ”€â”€ raw/                      # åŸå§‹æ•°æ®
    â”œâ”€â”€ processed/                 # å¤„ç†åæ•°æ®
    â”œâ”€â”€ train_data.json           # è®­ç»ƒé›†
    â”œâ”€â”€ val_data.json             # éªŒè¯é›†
    â””â”€â”€ test_data.json            # æµ‹è¯•é›†
```

---

## å‡†å¤‡è®­ç»ƒæ•°æ®

### 2.1 æ•°æ®æ¥æº

1. **å†å²äº¤æ˜“è®°å½•**ï¼šæ•°æ®åº“ä¸­çš„äº¤æ˜“æ—¥å¿—
2. **K çº¿æ•°æ®**ï¼šOKX API è·å–çš„å†å²æ•°æ®
3. **AI ä¿¡å·è®°å½•**ï¼šæ¯æ¬¡ AI åˆ†æçš„ market_data å’Œç»“æœ
4. **å¸‚åœºæ ‡æ³¨**ï¼šä¸“ä¸šçš„äº¤æ˜“æ ‡æ³¨

### 2.2 æ•°æ®æ ¼å¼è¦æ±‚

**Instruction Tuning æ ¼å¼**ï¼š

```json
{
    "instruction": "åˆ†æå¸‚åœºæ•°æ®ï¼Œç»™å‡ºäº¤æ˜“å»ºè®®",
    "input": "å½“å‰ä»·æ ¼: 50000\nRSI(14): 45\nMACD: é‡‘å‰\nè¶‹åŠ¿: ä¸Šå‡\nATR%: 2.5%",
    "output": "BUY - ä¸Šå‡è¶‹åŠ¿ + RSIå¥åº·ï¼Œå»ºè®®ä¹°å…¥"
}
```

**ChatML æ ¼å¼**ï¼š

```json
{
    "messages": [
        {
            "role": "system",
            "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“åŠ©æ‰‹"
        },
        {
            "role": "user",
            "content": "BTCä»·æ ¼50000ï¼ŒRSI45ï¼ŒMACDé‡‘å‰ï¼Œå»ºè®®ï¼Ÿ"
        },
        {
            "role": "assistant",
            "content": "å»ºè®® BUYï¼Œç½®ä¿¡åº¦ 75%"
        }
    ]
}
```

### 2.3 æ•°æ®æ”¶é›†è„šæœ¬

åˆ›å»º `scripts/training/prepare_data.py`ï¼š

```python
"""
æ•°æ®æ”¶é›†è„šæœ¬
ä»äº¤æ˜“æ•°æ®åº“ä¸­æ”¶é›†è®­ç»ƒæ•°æ®
"""
import json
import sqlite3
from datetime import datetime, timedelta
from typing import List, Dict, Any
import pandas as pd

def collect_training_data(
    db_path: str = "trades.db",
    output_path: str = "training_data/raw/collected_data.json",
    limit: int = 1000
) -> List[Dict[str, Any]]:
    """
    ä»æ•°æ®åº“æ”¶é›†è®­ç»ƒæ•°æ®
    
    Args:
        db_path: æ•°æ®åº“è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        limit: æœ€å¤§æ”¶é›†æ¡æ•°
    
    Returns:
        è®­ç»ƒæ•°æ®åˆ—è¡¨
    """
    try:
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        
        query = """
        SELECT 
            t.timestamp,
            t.symbol,
            t.entry_price,
            t.exit_price,
            t.pnl_percent,
            s.market_data,
            s.ai_signal,
            s.signal_confidence,
            t.action_taken,
            t.position_size,
            t.leverage,
            t.status
        FROM trades t
        LEFT JOIN signals s ON t.timestamp = s.timestamp
        WHERE t.pnl_percent IS NOT NULL
        AND t.status = 'closed'
        ORDER BY t.timestamp DESC
        LIMIT ?
        """
        
        cursor.execute(query, (limit,))
        records = cursor.fetchall()
        conn.close()
        
        training_data = []
        
        for record in records:
            (
                timestamp, symbol, entry_price, exit_price, pnl,
                market_data_str, ai_signal, confidence, action,
                position_size, leverage, status
            ) = record
            
            # æ ‡è®°ç»“æœ
            if pnl and pnl > 5:
                result_label = "ç›ˆåˆ©ä¿¡å·"
                result_quality = "high"
            elif pnl and pnl < -3:
                result_label = "äºæŸä¿¡å·"
                result_quality = "low"
            elif pnl and pnl > 0:
                result_label = "å°å¹…ç›ˆåˆ©ä¿¡å·"
                result_quality = "medium"
            else:
                result_label = "æŒå¹³ä¿¡å·"
                result_quality = "medium"
            
            # æ„å»ºè®­ç»ƒæ ·æœ¬
            sample = {
                "timestamp": timestamp,
                "symbol": symbol,
                "instruction": f"åˆ†æ{symbol}çš„å¸‚åœºæ•°æ®ï¼Œç»™å‡ºäº¤æ˜“å»ºè®®ã€‚ç»“æœæ ‡æ³¨ï¼š{result_label}",
                "input": market_data_str or "",
                "output": f"ä¿¡å·ï¼š{ai_signal or 'HOLD'}ï¼Œç½®ä¿¡åº¦ï¼š{confidence or 70}%ï¼Œå»ºè®®æ“ä½œï¼š{action or 'è§‚å¯Ÿ'}",
                "metadata": {
                    "entry_price": entry_price,
                    "exit_price": exit_price,
                    "pnl_percent": pnl,
                    "result_quality": result_quality,
                    "position_size": position_size,
                    "leverage": leverage
                }
            }
            training_data.append(sample)
        
        # ä¿å­˜
        import os
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å·²æ”¶é›† {len(training_data)} æ¡è®­ç»ƒæ•°æ®")
        print(f"ğŸ“ ä¿å­˜è‡³: {output_path}")
        
        return training_data
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ”¶é›†å¤±è´¥: {e}")
        return []


def collect_from_market_data(
    market_data_list: List[Dict[str, Any]],
    output_path: str = "training_data/raw/market_data.json"
) -> List[Dict[str, Any]]:
    """
    ä»å®æ—¶å¸‚åœºæ•°æ®ç”Ÿæˆè®­ç»ƒæ ·æœ¬
    
    Args:
        market_data_list: å¸‚åœºæ•°æ®åˆ—è¡¨
        output_path: è¾“å‡ºè·¯å¾„
    
    Returns:
        è®­ç»ƒæ•°æ®åˆ—è¡¨
    """
    training_data = []
    
    for market_data in market_data_list:
        technical = market_data.get("technical", {})
        price = market_data.get("price", 0)
        rsi = technical.get("rsi", 50)
        trend_dir = technical.get("trend_direction", "neutral")
        trend_strength = technical.get("trend_strength", 0)
        
        # è‡ªåŠ¨ç”Ÿæˆæ ‡æ³¨
        if rsi < 35 and trend_dir == "bullish":
            suggested_signal = "BUY"
            reason = "è¶…å– + ä¸Šå‡è¶‹åŠ¿"
        elif rsi > 65 and trend_dir == "bearish":
            suggested_signal = "SELL"
            reason = "è¶…ä¹° + ä¸‹é™è¶‹åŠ¿"
        elif trend_dir == "bullish" and trend_strength > 0.6:
            suggested_signal = "BUY"
            reason = "å¼ºä¸Šå‡è¶‹åŠ¿"
        elif trend_dir == "bearish" and trend_strength > 0.6:
            suggested_signal = "SELL"
            reason = "å¼ºä¸‹é™è¶‹åŠ¿"
        else:
            suggested_signal = "HOLD"
            reason = "å¸‚åœºéœ‡è¡ï¼Œå»ºè®®è§‚æœ›"
        
        sample = {
            "instruction": f"åˆ†æ{market_data.get('symbol', 'BTC')}å¸‚åœºæ•°æ®ï¼Œç»™å‡ºäº¤æ˜“å»ºè®®",
            "input": json.dumps(market_data, ensure_ascii=False),
            "output": f"å»ºè®®ï¼š{suggested_signal}ï¼Œç†ç”±ï¼š{reason}",
            "metadata": {
                "source": "auto_generated",
                "price": price,
                "rsi": rsi,
                "trend": trend_dir
            }
        }
        training_data.append(sample)
    
    # ä¿å­˜
    import os
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(training_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å·²ç”Ÿæˆ {len(training_data)} æ¡è®­ç»ƒæ•°æ®")
    return training_data


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    data = collect_training_data(
        db_path="trades.db",
        output_path="training_data/raw/collected_data.json",
        limit=500
    )
```

### 2.4 æ•°æ®å¢å¼º

åˆ›å»º `scripts/training/augment_data.py`ï¼š

```python
"""
æ•°æ®å¢å¼ºè„šæœ¬
å¯¹å·²æœ‰æ•°æ®è¿›è¡Œå¢å¼ºï¼Œæ‰©å……æ•°æ®é‡
"""
import json
import random
from typing import List, Dict, Any

VARIATIONS = [
    ("ç®€æ´æ¨¡å¼", "è¯·ç®€æ´å›ç­”"),
    ("è¯¦ç»†æ¨¡å¼", "è¯·è¯¦ç»†åˆ†æåŸå› "),
    ("æŠ€æœ¯åˆ†ææ¨¡å¼", "é‡ç‚¹åˆ†ææŠ€æœ¯æŒ‡æ ‡"),
    ("é£é™©æç¤ºæ¨¡å¼", "è¯·åŒ…å«é£é™©æç¤º"),
    ("æ–°æ‰‹å‹å¥½æ¨¡å¼", "è¯·è§£é‡Šä¸“ä¸šæœ¯è¯­")
]

SIGNALS = ["BUY", "SELL", "HOLD"]

def augment_data(
    input_path: str,
    output_path: str,
    multiplier: int = 3
) -> List[Dict[str, Any]]:
    """
    æ•°æ®å¢å¼º
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        multiplier: å¢å¼ºå€æ•°
    
    Returns:
        å¢å¼ºåçš„æ•°æ®åˆ—è¡¨
    """
    with open(input_path, 'r', encoding='utf-8') as f:
        original_data = json.load(f)
    
    augmented = []
    
    for sample in original_data:
        base_instruction = sample["instruction"]
        base_input = sample["input"]
        base_output = sample["output"]
        
        for i in range(multiplier):
            variation_type, prefix = VARIATIONS[i % len(VARIATIONS)]
            
            # éšæœºå˜æ¢ä¿¡å·ï¼ˆä¿æŒè¾“å‡ºä¸€è‡´æ€§ï¼‰
            if random.random() > 0.7:
                # è½»å¾®ä¿®æ”¹è¾“å…¥æ ¼å¼
                new_input = f"[{variation_type}]\n{base_input}"
            else:
                new_input = base_input
            
            augmented_sample = {
                "instruction": f"{prefix} {base_instruction}",
                "input": new_input,
                "output": base_output,
                "metadata": {
                    **sample.get("metadata", {}),
                    "augmented": True,
                    "variation_type": variation_type
                }
            }
            augmented.append(augmented_sample)
    
    # ä¿å­˜
    import os
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(augmented, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å¢å¼ºå®Œæˆ: {len(original_data)} â†’ {len(augmented)} æ¡")
    return augmented


def add_noise(
    input_path: str,
    output_path: str,
    noise_ratio: float = 0.1
) -> List[Dict[str, Any]]:
    """
    æ·»åŠ å™ªå£°æ•°æ®ï¼ˆæé«˜æ¨¡å‹é²æ£’æ€§ï¼‰
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        noise_ratio: å™ªå£°æ¯”ä¾‹
    """
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    noisy_data = []
    
    for sample in data:
        # æ·»åŠ å™ªå£°åˆ°æ•°å€¼
        input_text = sample["input"]
        
        # éšæœºä¿®æ”¹æ•°å€¼ï¼ˆÂ±10%ï¼‰
        import re
        def modify_number(match):
            value = float(match.group())
            noise = value * noise_ratio * (random.random() * 2 - 1)
            return str(int(value + noise))
        
        noisy_text = re.sub(r'\d+', modify_number, input_text)
        
        noisy_sample = {
            **sample,
            "input": noisy_text,
            "metadata": {
                **sample.get("metadata", {}),
                "noisy": True
            }
        }
        noisy_data.append(noisy_sample)
    
    # ä¿å­˜
    import os
    os.makedirs(os.path.dirname(output_path), exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(noisy_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… å™ªå£°æ•°æ®æ·»åŠ å®Œæˆ: {len(noisy_data)} æ¡")
    return noisy_data


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    augment_data(
        "training_data/raw/collected_data.json",
        "training_data/raw/augmented_data.json",
        multiplier=3
    )
```

### 2.5 æ•°æ®åˆ†å‰²

åˆ›å»º `scripts/training/split_data.py`ï¼š

```python
"""
æ•°æ®åˆ†å‰²è„šæœ¬
å°†æ•°æ®åˆ†å‰²ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
"""
import json
from sklearn.model_selection import train_test_split
from typing import Tuple

def split_data(
    input_path: str,
    train_path: str,
    val_path: str,
    test_path: str,
    test_size: float = 0.2,
    val_size: float = 0.1,
    random_state: int = 42
) -> Tuple[list, list, list]:
    """
    åˆ†å‰²æ•°æ®
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        train_path: è®­ç»ƒé›†è¾“å‡ºè·¯å¾„
        val_path: éªŒè¯é›†è¾“å‡ºè·¯å¾„
        test_path: æµ‹è¯•é›†è¾“å‡ºè·¯å¾„
        test_size: æµ‹è¯•é›†æ¯”ä¾‹
        val_size: éªŒè¯é›†æ¯”ä¾‹ï¼ˆç›¸å¯¹äºéæµ‹è¯•æ•°æ®ï¼‰
        random_state: éšæœºç§å­
    
    Returns:
        è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
    """
    with open(input_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“Š æ€»æ•°æ®é‡: {len(data)} æ¡")
    
    # åˆ†å‰²
    train_data, temp_data = train_test_split(
        data,
        test_size=test_size,
        random_state=random_state
    )
    
    val_data, test_data = train_test_split(
        temp_data,
        test_size=val_size / (1 - test_size),  # è°ƒæ•´éªŒè¯é›†æ¯”ä¾‹
        random_state=random_state
    )
    
    # ä¿å­˜
    import os
    os.makedirs(os.path.dirname(train_path), exist_ok=True)
    
    with open(train_path, 'w', encoding='utf-8') as f:
        json.dump(train_data, f, ensure_ascii=False, indent=2)
    
    with open(val_path, 'w', encoding='utf-8') as f:
        json.dump(val_data, f, ensure_ascii=False, indent=2)
    
    with open(test_path, 'w', encoding='utf-8') as f:
        json.dump(test_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… æ•°æ®åˆ†å‰²å®Œæˆ:")
    print(f"   è®­ç»ƒé›†: {len(train_data)} æ¡ ({len(train_data)/len(data)*100:.1f}%)")
    print(f"   éªŒè¯é›†: {len(val_data)} æ¡ ({len(val_data)/len(data)*100:.1f}%)")
    print(f"   æµ‹è¯•é›†: {len(test_data)} æ¡ ({len(test_data)/len(data)*100:.1f}%)")
    
    return train_data, val_data, test_data


def analyze_data_balance(
    data_path: str,
    label_key: str = "output"
) -> dict:
    """
    åˆ†ææ•°æ®åˆ†å¸ƒ
    
    Args:
        data_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        label_key: æ ‡ç­¾å­—æ®µ
    
    Returns:
        åˆ†å¸ƒç»Ÿè®¡
    """
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ç»Ÿè®¡ä¿¡å·åˆ†å¸ƒ
    signal_counts = {"BUY": 0, "SELL": 0, "HOLD": 0}
    
    for sample in data:
        output = sample.get(label_key, "")
        if "BUY" in output:
            signal_counts["BUY"] += 1
        elif "SELL" in output:
            signal_counts["SELL"] += 1
        else:
            signal_counts["HOLD"] += 1
    
    total = len(data)
    print(f"ğŸ“Š æ•°æ®åˆ†å¸ƒ:")
    for signal, count in signal_counts.items():
        ratio = count / total * 100
        bar = "â–ˆ" * int(ratio / 2)
        print(f"   {signal}: {count} ({ratio:.1f}%) {bar}")
    
    return signal_counts


if __name__ == "__main__":
    split_data(
        "training_data/raw/augmented_data.json",
        "training_data/train_data.json",
        "training_data/val_data.json",
        "training_data/test_data.json"
    )
    
    analyze_data_balance("training_data/train_data.json")
```

---

## æ•°æ®æ ¼å¼åŒ–

### 3.1 è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼

åˆ›å»º `scripts/training/convert_format.py`ï¼š

```python
"""
æ•°æ®æ ¼å¼è½¬æ¢è„šæœ¬
è½¬æ¢ä¸ºæ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ ¼å¼
"""
import json
from datasets import Dataset
from typing import List, Dict, Any

def convert_to_chatml(
    data: List[Dict[str, Any]],
    system_prompt: str = None
) -> List[Dict[str, Any]]:
    """
    è½¬æ¢ä¸º ChatML æ ¼å¼
    
    Args:
        data: åŸå§‹æ•°æ®åˆ—è¡¨
        system_prompt: ç³»ç»Ÿæç¤ºè¯
    
    Returns:
        ChatML æ ¼å¼æ•°æ®
    """
    default_system = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“åŠ©æ‰‹ã€‚æ ¹æ®æŠ€æœ¯æŒ‡æ ‡å’Œå¸‚åœºæ•°æ®ï¼Œç»™å‡ºç®€æ´æ˜ç¡®çš„äº¤æ˜“å»ºè®®ã€‚

è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
1. ä»…ç»™å‡ºä¿¡å·ï¼šBUY / SELL / HOLD
2. æä¾›ç½®ä¿¡åº¦ï¼š0-100%
3. è¯´æ˜ä¸»è¦ç†ç”±
4. åŒ…å«é£é™©æç¤ºï¼ˆå¦‚é€‚ç”¨ï¼‰"""
    
    if system_prompt is None:
        system_prompt = default_system
    
    chatml_data = []
    
    for item in data:
        messages = [
            {"role": "system", "content": system_prompt},
            {
                "role": "user",
                "content": f"{item['instruction']}\n\nå¸‚åœºæ•°æ®ï¼š\n{item['input']}"
            },
            {"role": "assistant", "content": item['output']}
        ]
        
        chatml_data.append({"messages": messages})
    
    return chatml_data


def convert_to_alpaca(
    data: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    è½¬æ¢ä¸º Alpaca æ ¼å¼
    
    Args:
        data: åŸå§‹æ•°æ®åˆ—è¡¨
    
    Returns:
        Alpaca æ ¼å¼æ•°æ®
    """
    alpaca_data = []
    
    for item in data:
        alpaca_data.append({
            "instruction": item['instruction'],
            "input": item['input'],
            "output": item['output']
        })
    
    return alpaca_data


def convert_to_huggingface_dataset(
    data: List[Dict[str, Any]],
    format_type: str = "chatml"
) -> Dataset:
    """
    è½¬æ¢ä¸º Hugging Face Dataset
    
    Args:
        data: åŸå§‹æ•°æ®åˆ—è¡¨
        format_type: æ ¼å¼ç±»å‹ ('chatml', 'alpaca')
    
    Returns:
        Hugging Face Dataset
    """
    if format_type == "chatml":
        converted = convert_to_chatml(data)
    else:
        converted = convert_to_alpaca(data)
    
    dataset = Dataset.from_list(converted)
    return dataset


def format_with_template(
    data: List[Dict[str, Any]],
    template_name: str = "qwen"
) -> List[Dict[str, Any]]:
    """
    ä½¿ç”¨ç‰¹å®šæ¨¡æ¿æ ¼å¼åŒ–æ•°æ®
    
    Args:
        data: åŸå§‹æ•°æ®åˆ—è¡¨
        template_name: æ¨¡æ¿åç§° ('qwen', 'llama', 'mistral')
    
    Returns:
        æ ¼å¼åŒ–åçš„æ•°æ®
    """
    templates = {
        "qwen": {
            "system": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“åŠ©æ‰‹ã€‚",
            "user_template": "### åˆ†æä»»åŠ¡\n{instruction}\n\n### å¸‚åœºæ•°æ®\n{input}",
            "assistant_template": "### äº¤æ˜“å»ºè®®\n{output}"
        },
        "llama": {
            "system": "[INST] ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“åŠ©æ‰‹ã€‚ [/INST]",
            "user_template": "[INST] {instruction}\n\n{input} [/INST]",
            "assistant_template": "[/INST] {output} [/INST]"
        },
        "mistral": {
            "system": "<s>System: ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“åŠ©æ‰‹ã€‚</s>",
            "user_template": "<s>User: {instruction}\n\n{input}</s>",
            "assistant_template": "<s>Assistant: {output}</s>"
        }
    }
    
    template = templates.get(template_name, templates["qwen"])
    formatted_data = []
    
    for item in data:
        formatted = {
            "messages": [
                {"role": "system", "content": template["system"]},
                {
                    "role": "user",
                    "content": template["user_template"].format(
                        instruction=item['instruction'],
                        input=item['input']
                    )
                },
                {
                    "role": "assistant",
                    "content": template["assistant_template"].format(
                        output=item['output']
                    )
                }
            ]
        }
        formatted_data.append(formatted)
    
    return formatted_data


def apply_chat_template(
    dataset: Dataset,
    tokenizer,
    max_length: int = 2048
) -> Dataset:
    """
    åº”ç”¨ tokenizer çš„ chat template
    
    Args:
        dataset: Hugging Face Dataset
        tokenizer: åˆ†è¯å™¨
        max_length: æœ€å¤§é•¿åº¦
    
    Returns:
        å¤„ç†åçš„ Dataset
    """
    def formatting_prompts_func(examples):
        texts = []
        for messages in examples["messages"]:
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
            texts.append(text)
        return {"text": texts}
    
    dataset = dataset.map(
        formatting_prompts_func,
        batched=True,
        remove_columns=["messages"]
    )
    
    # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
    def truncate(example):
        if len(example["input_ids"]) > max_length:
            example["input_ids"] = example["input_ids"][:max_length]
            example["attention_mask"] = example["attention_mask"][:max_length]
        return example
    
    dataset = dataset.map(truncate)
    
    return dataset


if __name__ == "__main__":
    # ä½¿ç”¨ç¤ºä¾‹
    with open("training_data/train_data.json", 'r') as f:
        train_data = json.load(f)
    
    # è½¬æ¢ä¸º ChatML æ ¼å¼
    chatml_data = convert_to_chatml(train_data)
    
    # è½¬æ¢ä¸º Dataset
    train_dataset = convert_to_huggingface_dataset(chatml_data, "chatml")
    
    print(f"âœ… æ•°æ®æ ¼å¼åŒ–å®Œæˆ: {len(train_dataset)} æ¡")
    print(f"ğŸ“ ç¤ºä¾‹:")
    print(train_dataset[0]["messages"][0])
```

### 3.2 è´¨é‡æ£€æŸ¥

åˆ›å»º `scripts/training/quality_check.py`ï¼š

```python
"""
æ•°æ®è´¨é‡æ£€æŸ¥è„šæœ¬
ç¡®ä¿è®­ç»ƒæ•°æ®è´¨é‡
"""
import json
from typing import List, Dict, Any
import re

class DataQualityChecker:
    """æ•°æ®è´¨é‡æ£€æŸ¥å™¨"""
    
    def __init__(self):
        self.errors = []
        self.warnings = []
    
    def check(self, data: List[Dict[str, Any]]) -> bool:
        """
        æ£€æŸ¥æ•°æ®è´¨é‡
        
        Args:
            data: æ•°æ®åˆ—è¡¨
        
        Returns:
            æ˜¯å¦é€šè¿‡æ‰€æœ‰æ£€æŸ¥
        """
        self.errors = []
        self.warnings = []
        
        for i, sample in enumerate(data):
            self._check_sample(sample, i)
        
        # è¾“å‡ºæ£€æŸ¥ç»“æœ
        print(f"\nğŸ“Š è´¨é‡æ£€æŸ¥ç»“æœ:")
        print(f"   æ€»æ ·æœ¬æ•°: {len(data)}")
        print(f"   é”™è¯¯æ•°: {len(self.errors)}")
        print(f"   è­¦å‘Šæ•°: {len(self.warnings)}")
        
        if self.errors:
            print(f"\nâŒ é”™è¯¯ (å¿…é¡»ä¿®å¤):")
            for error in self.errors[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"   - {error}")
        
        if self.warnings:
            print(f"\nâš ï¸ è­¦å‘Š (å»ºè®®ä¿®å¤):")
            for warning in self.warnings[:10]:
                print(f"   - {warning}")
        
        return len(self.errors) == 0
    
    def _check_sample(self, sample: Dict[str, Any], index: int):
        """æ£€æŸ¥å•ä¸ªæ ·æœ¬"""
        # æ£€æŸ¥å¿…è¦å­—æ®µ
        for field in ["instruction", "input", "output"]:
            if field not in sample:
                self.errors.append(f"[{index}] ç¼ºå°‘å­—æ®µ: {field}")
                return
        
        # æ£€æŸ¥ instruction é•¿åº¦
        if len(sample["instruction"]) < 5:
            self.warnings.append(f"[{index}] instruction è¿‡çŸ­")
        
        # æ£€æŸ¥ input é•¿åº¦
        if len(sample["input"]) < 10:
            self.warnings.append(f"[{index}] input è¿‡çŸ­")
        
        # æ£€æŸ¥ output æ ¼å¼
        output = sample["output"]
        if not any(signal in output for signal in ["BUY", "SELL", "HOLD"]):
            self.errors.append(f"[{index}] output ç¼ºå°‘æœ‰æ•ˆä¿¡å·: {output[:50]}...")
        
        # æ£€æŸ¥ JSON æ ¼å¼
        try:
            if sample.get("input", "").startswith("{"):
                json.loads(sample["input"])
        except json.JSONDecodeError:
            self.warnings.append(f"[{index}] input ä¸æ˜¯æœ‰æ•ˆ JSON")


def check_label_distribution(data: List[Dict[str, Any]]) -> dict:
    """
    æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    
    Args:
        data: æ•°æ®åˆ—è¡¨
    
    Returns:
        åˆ†å¸ƒç»Ÿè®¡
    """
    distribution = {"BUY": 0, "SELL": 0, "HOLD": 0}
    
    for sample in data:
        output = sample.get("output", "")
        if "BUY" in output:
            distribution["BUY"] += 1
        elif "SELL" in output:
            distribution["SELL"] += 1
        else:
            distribution["HOLD"] += 1
    
    total = len(data)
    print("\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
    for label, count in distribution.items():
        ratio = count / total * 100
        bar = "â–ˆ" * int(ratio / 2)
        print(f"   {label}: {count:4d} ({ratio:5.1f}%) {bar}")
    
    # æ£€æŸ¥æ˜¯å¦å¹³è¡¡
    max_count = max(distribution.values())
    min_count = min(distribution.values())
    balance_ratio = min_count / max_count if max_count > 0 else 0
    
    if balance_ratio < 0.3:
        print(f"\nâš ï¸ è­¦å‘Š: æ•°æ®ä¸å¹³è¡¡ (æ¯”ä¾‹: {balance_ratio:.2f})")
        print("   å»ºè®®è¿›è¡Œæ•°æ®å¢å¼ºæˆ–é‡é‡‡æ ·")
    
    return distribution


def remove_duplicates(data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å»é™¤é‡å¤æ•°æ®
    
    Args:
        data: æ•°æ®åˆ—è¡¨
    
    Returns:
        å»é‡åçš„æ•°æ®åˆ—è¡¨
    """
    seen = set()
    unique_data = []
    
    for sample in data:
        # ä½¿ç”¨ input ä½œä¸ºå»é‡é”®
        key = sample.get("input", "")
        if key not in seen:
            seen.add(key)
            unique_data.append(sample)
    
    removed = len(data) - len(unique_data)
    print(f"\nâœ… å»é‡å®Œæˆ: {removed} æ¡é‡å¤æ•°æ®è¢«ç§»é™¤")
    print(f"   åŸå§‹: {len(data)} â†’ å»é‡å: {len(unique_data)}")
    
    return unique_data


if __name__ == "__main__":
    with open("training_data/train_data.json", 'r') as f:
        data = json.load(f)
    
    # è´¨é‡æ£€æŸ¥
    checker = DataQualityChecker()
    checker.check(data)
    
    # æ ‡ç­¾åˆ†å¸ƒ
    check_label_distribution(data)
    
    # å»é‡
    unique_data = remove_duplicates(data)
```

---

## é…ç½®å¹¶å¯åŠ¨ LoRA è®­ç»ƒ

### 4.1 è®­ç»ƒè„šæœ¬

åˆ›å»º `scripts/training/train_lora.py`ï¼š

```python
"""
LoRA è®­ç»ƒè„šæœ¬
ä½¿ç”¨ Qwen2.5-7B-Instruct è¿›è¡Œå¾®è°ƒ
"""
import os
import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    DataCollatorForSeq2Seq,
    BitsAndBytesConfig
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType,
    prepare_model_for_int8_training
)
from unsloth import UnslothModel
from datasets import load_dataset
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============ é…ç½®å‚æ•° ============
MODEL_NAME = "Qwen/Qwen2.5-7B-Instruct"  # æˆ– "meta-llama/Llama-3-8B-Instruct"
OUTPUT_DIR = "./trading-llm-lora"
MAX_SEQ_LENGTH = 2048

# LoRA å‚æ•°
LORA_R = 16
LORA_ALPHA = 32
LORA_DROPOUT = 0.05
LORA_TARGET_MODULES = ["q_proj", "k_proj", "v_proj", "o_proj"]

# è®­ç»ƒå‚æ•°
NUM_TRAIN_EPOCHS = 3
PER_DEVICE_TRAIN_BATCH_SIZE = 2
GRADIENT_ACCUMULATION_STEPS = 4
LEARNING_RATE = 1e-4
MAX_STEPS = 1000
EVAL_STEPS = 50
SAVE_STEPS = 100
LOGGING_STEPS = 10


def print_model_info(model):
    """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
    trainable_params = 0
    all_params = 0
    
    for _, param in model.named_parameters():
        num_params = param.numel()
        all_params += num_params
        if param.requires_grad:
            trainable_params += num_params
    
    print(f"\n{'='*50}")
    print(f"æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°: {all_params:,}")
    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
    print(f"  è®­ç»ƒæ¯”ä¾‹: {trainable_params/all_params*100:.2f}%")
    print(f"{'='*50}\n")


def main():
    """ä¸»å‡½æ•°"""
    
    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    logger.info("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    model, tokenizer = UnslothModel.from_pretrained(
        MODEL_NAME,
        max_seq_length=MAX_SEQ_LENGTH,
        dtype=torch.float16,  # æˆ– torch.bfloat16
        load_in_4bit=True,    # 4bit é‡åŒ–
    )
    
    logger.info(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {MODEL_NAME}")
    logger.info(f"   åºåˆ—é•¿åº¦: {MAX_SEQ_LENGTH}")
    
    # 2. é…ç½® LoRA
    logger.info("æ­£åœ¨é…ç½® LoRA...")
    
    lora_config = LoraConfig(
        r=LORA_R,
        lora_alpha=LORA_ALPHA,
        target_modules=LORA_TARGET_MODULES,
        lora_dropout=LORA_DROPOUT,
        bias="none",
        task_type=TaskType.CAUSAL_LM,
    )
    
    model = get_peft_model(model, lora_config)
    
    print_model_info(model)
    
    # 3. åŠ è½½æ•°æ®é›†
    logger.info("æ­£åœ¨åŠ è½½æ•°æ®é›†...")
    
    train_dataset = load_dataset("json", data_files="training_data/train_data.json", split="train")
    val_dataset = load_dataset("json", data_files="training_data/val_data.json", split="train")
    
    logger.info(f"âœ… è®­ç»ƒé›†: {len(train_dataset)} æ¡")
    logger.info(f"âœ… éªŒè¯é›†: {len(val_dataset)} æ¡")
    
    # 4. æ ¼å¼åŒ–æ•°æ®
    logger.info("æ­£åœ¨æ ¼å¼åŒ–æ•°æ®...")
    
    def formatting_prompts_func(examples):
        texts = []
        for messages in examples["messages"]:
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
            texts.append(text)
        return {"text": texts}
    
    train_dataset = train_dataset.map(
        formatting_prompts_func,
        batched=True,
        remove_columns=["messages"]
    )
    
    val_dataset = val_dataset.map(
        formatting_prompts_func,
        batched=True,
        remove_columns=["messages"]
    )
    
    # 5. é…ç½®è®­ç»ƒå‚æ•°
    logger.info("æ­£åœ¨é…ç½®è®­ç»ƒå‚æ•°...")
    
    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=NUM_TRAIN_EPOCHS,
        per_device_train_batch_size=PER_DEVICE_TRAIN_BATCH_SIZE,
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,
        learning_rate=LEARNING_RATE,
        weight_decay=0.01,
        warmup_steps=10,
        max_steps=MAX_STEPS,
        logging_steps=LOGGING_STEPS,
        eval_steps=EVAL_STEPS,
        save_steps=SAVE_STEPS,
        eval_strategy="steps",
        save_strategy="steps",
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to="tensorboard",
        fp16=True,  # æ··åˆç²¾åº¦
        optim="adamw_8bit",  # 8bit ä¼˜åŒ–å™¨
        lr_scheduler_type="linear",
        save_total_limit=3,  # æœ€å¤šä¿å­˜3ä¸ªcheckpoint
    )
    
    # 6. åˆ›å»º DataCollator
    data_collator = DataCollatorForSeq2Seq(
        tokenizer=tokenizer,
        padding=True,
        return_tensors="pt"
    )
    
    # 7. åˆ›å»º Trainer
    logger.info("æ­£åœ¨åˆ›å»º Trainer...")
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        data_collator=data_collator,
    )
    
    # 8. å¼€å§‹è®­ç»ƒ
    logger.info("\n" + "="*50)
    logger.info("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    logger.info("="*50)
    
    train_result = trainer.train()
    
    # è¾“å‡ºè®­ç»ƒç»“æœ
    metrics = train_result.metrics
    logger.info(f"\nğŸ“Š è®­ç»ƒå®Œæˆ:")
    logger.info(f"   æ€»æ­¥æ•°: {metrics['train_steps']}")
    logger.info(f"   æœ€ç»ˆæŸå¤±: {metrics['train_loss']:.4f}")
    logger.info(f"   å­¦ä¹ ç‡: {training_args.learning_rate}")
    
    # 9. ä¿å­˜æ¨¡å‹
    logger.info("\næ­£åœ¨ä¿å­˜æ¨¡å‹...")
    
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    model.save_pretrained(f"{OUTPUT_DIR}/final_lora")
    tokenizer.save_pretrained(f"{OUTPUT_DIR}/final_lora")
    
    # ä¿å­˜è®­ç»ƒé…ç½®
    config_info = {
        "model_name": MODEL_NAME,
        "lora_r": LORA_R,
        "lora_alpha": LORA_ALPHA,
        "max_seq_length": MAX_SEQ_LENGTH,
        "train_samples": len(train_dataset),
        "eval_samples": len(val_dataset),
        "final_loss": metrics.get('train_loss'),
    }
    
    with open(f"{OUTPUT_DIR}/config.json", 'w') as f:
        json.dump(config_info, f, indent=2)
    
    logger.info(f"\nâœ… è®­ç»ƒå®Œæˆï¼")
    logger.info(f"ğŸ“ æ¨¡å‹ä¿å­˜åœ¨: {OUTPUT_DIR}/final_lora")
    logger.info(f"ğŸ“Š è®­ç»ƒæ—¥å¿—: tensorboard --logdir {OUTPUT_DIR}")


if __name__ == "__main__":
    main()
```

### 4.2 å¯åŠ¨è®­ç»ƒ

```bash
# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¯é€‰ï¼Œæå‡ç¨³å®šæ€§ï¼‰
export CUDA_VISIBLE_DEVICES=0
export TOKENIZERS_PARALLELISM=false

# åˆ›å»ºè¾“å‡ºç›®å½•
mkdir -p trading-llm-lora

# å¯åŠ¨è®­ç»ƒ
python scripts/training/train_lora.py
```

### 4.3 æ˜¾å­˜ä¼˜åŒ–é…ç½®

å¦‚æœé‡åˆ°æ˜¾å­˜ä¸è¶³é—®é¢˜ï¼Œä¿®æ”¹ `train_lora.py` ä¸­çš„é…ç½®ï¼š

```python
# æ–¹æ¡ˆ1ï¼šæ›´æ¿€è¿›çš„é‡åŒ–
load_in_4bit=True   # 4bit é‡åŒ–ï¼ˆæ›´çœæ˜¾å­˜ï¼‰
load_in_8bit=False

# æ–¹æ¡ˆ2ï¼šå‡å°åºåˆ—é•¿åº¦
MAX_SEQ_LENGTH = 1024  # ä» 2048 é™åˆ° 1024

# æ–¹æ¡ˆ3ï¼šå‡å° batch size
PER_DEVICE_TRAIN_BATCH_SIZE = 1  # ä» 2 é™åˆ° 1

# æ–¹æ¡ˆ4ï¼šå¢å¤§æ¢¯åº¦ç´¯ç§¯
GRADIENT_ACCUMULATION_STEPS = 8  # ä» 4 å¢åˆ° 8

# æ–¹æ¡ˆ5ï¼šä½¿ç”¨ DeepSpeed ZeROï¼ˆé«˜çº§ï¼‰
# åœ¨å‘½ä»¤è¡Œæ·»åŠ 
# deepspeed --num_gpus=2 scripts/training/train_lora.py
```

### 4.4 ç›‘æ§è®­ç»ƒ

```bash
# ç»ˆç«¯1ï¼šå¯åŠ¨è®­ç»ƒ
python scripts/training/train_lora.py

# ç»ˆç«¯2ï¼šç›‘æ§æ˜¾å­˜
nvidia-smi -l 1

# ç»ˆç«¯3ï¼šä½¿ç”¨ TensorBoard
pip install tensorboard
tensorboard --logdir ./trading-llm-lora --port 6006
```

---

## æ¨¡å‹æµ‹è¯•å’Œè¯„ä¼°

### 5.1 æ¨ç†æµ‹è¯•

åˆ›å»º `scripts/training/test_model.py`ï¼š

```python
"""
æ¨¡å‹æµ‹è¯•è„šæœ¬
æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹æ•ˆæœ
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
from typing import Dict, List, Any

# é…ç½®
MODEL_PATH = "./trading-llm-lora/final_lora"
BASE_MODEL = "Qwen/Qwen2.5-7B-Instruct"

def load_model():
    """åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹"""
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        load_in_4bit=True,
        device_map="auto",
    )
    
    model = PeftModel.from_pretrained(base_model, MODEL_PATH)
    
    print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    return model, tokenizer


def build_prompt(market_data: Dict[str, Any]) -> str:
    """æ„å»ºæç¤ºè¯"""
    technical = market_data.get("technical", {})
    
    prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“åŠ©æ‰‹ã€‚

å½“å‰å¸‚åœºæ•°æ®ï¼š
- äº¤æ˜“å¯¹ï¼š{market_data.get('symbol', 'BTC/USDT')}
- å½“å‰ä»·æ ¼ï¼š{market_data.get('price', 0)}
- 24hæ¶¨è·Œå¹…ï¼š{market_data.get('change_percent', 0)}%
- RSI(14)ï¼š{technical.get('rsi', 50)}
- MACDçŠ¶æ€ï¼š{technical.get('macd_state', 'normal')}
- è¶‹åŠ¿æ–¹å‘ï¼š{technical.get('trend_direction', 'neutral')}
- è¶‹åŠ¿å¼ºåº¦ï¼š{technical.get('trend_strength', 0)}
- ATR%ï¼š{technical.get('atr_percent', 0)}
- ä»·æ ¼ä½ç½®ï¼š{technical.get('bb_position', 0)}%

è¯·åˆ†æä»¥ä¸Šæ•°æ®ï¼Œç»™å‡ºäº¤æ˜“ä¿¡å·ï¼ˆä»…é™ï¼šBUY / SELL / HOLDï¼‰ï¼Œå¹¶è¯´æ˜ç†ç”±ã€‚"""
    
    return prompt


def generate_signal(model, tokenizer, market_data: Dict[str, Any]) -> Dict[str, Any]:
    """ç”Ÿæˆäº¤æ˜“ä¿¡å·"""
    
    prompt = build_prompt(market_data)
    
    messages = [{"role": "user", "content": prompt}]
    
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=256,
        temperature=0.1,  # ä½æ¸©åº¦ï¼Œæ›´ç¨³å®šçš„è¾“å‡º
        top_p=0.9,
        do_sample=False,  # äº¤æ˜“åœºæ™¯ç”¨ greedy
    )
    
    response = tokenizer.decode(
        outputs[0],
        skip_special_tokens=True
    )
    
    # è§£æç»“æœ
    full_response = response
    
    # æå– assistant éƒ¨åˆ†
    if "assistant" in response:
        signal_text = response.split("assistant")[-1].strip()
    else:
        signal_text = response
    
    # æå–ä¿¡å·
    signal = "HOLD"
    if "BUY" in signal_text.upper() and "SELL" not in signal_text.upper():
        signal = "BUY"
    elif "SELL" in signal_text.upper():
        signal = "SELL"
    
    # æå–ç½®ä¿¡åº¦
    confidence = 70
    import re
    match = re.search(r'(\d+)%?', signal_text)
    if match:
        confidence = min(100, max(0, int(match.group(1))))
    
    return {
        "signal": signal,
        "confidence": confidence,
        "full_response": signal_text,
        "market_data": market_data
    }


def test_on_samples(model, tokenizer, test_data_path: str):
    """åœ¨æµ‹è¯•é›†ä¸Šæµ‹è¯•"""
    
    with open(test_data_path, 'r') as f:
        test_data = json.load(f)
    
    print(f"\nğŸ“Š æµ‹è¯•æ ·æœ¬æ•°: {len(test_data)}")
    
    correct = 0
    results = []
    
    for i, sample in enumerate(test_data[:50]):  # æµ‹è¯•å‰50æ¡
        # è§£æ market_data
        market_data = json.loads(sample["input"])
        
        result = generate_signal(model, tokenizer, market_data)
        
        # å¯¹æ¯”é¢„æœŸ
        expected = sample["output"]
        actual = result["signal"]
        
        is_correct = expected.split()[0] == actual
        if is_correct:
            correct += 1
        
        results.append({
            "expected": expected,
            "actual": result,
            "correct": is_correct
        })
        
        print(f"[{i+1}] é¢„æœŸ: {expected.split()[0]:4s} | å®é™…: {actual:4s} | {'âœ…' if is_correct else 'âŒ'}")
    
    accuracy = correct / min(50, len(test_data))
    print(f"\nğŸ“Š æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.2%} ({correct}/{min(50, len(test_data))})")
    
    return accuracy, results


def interactive_test(model, tokenizer):
    """äº¤äº’å¼æµ‹è¯•"""
    
    print("\n" + "="*50)
    print("ğŸ§ª äº¤äº’å¼æµ‹è¯•")
    print("="*50)
    print("è¾“å…¥å¸‚åœºæ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰ï¼Œè¾“å…¥ q é€€å‡º\n")
    
    while True:
        user_input = input("å¸‚åœºæ•°æ® (JSON): ").strip()
        
        if user_input.lower() == 'q':
            break
        
        try:
            market_data = json.loads(user_input)
            result = generate_signal(model, tokenizer, market_data)
            print(f"\nğŸ“¤ ç»“æœ:")
            print(f"   ä¿¡å·: {result['signal']}")
            print(f"   ç½®ä¿¡åº¦: {result['confidence']}%")
            print(f"   è¯¦ç»†: {result['full_response'][:200]}...")
            print()
        except json.JSONDecodeError:
            print("âŒ JSON æ ¼å¼é”™è¯¯\n")


if __name__ == "__main__":
    model, tokenizer = load_model()
    
    # è‡ªåŠ¨æµ‹è¯•
    accuracy, results = test_on_samples(
        model, tokenizer,
        "training_data/test_data.json"
    )
    
    # äº¤äº’å¼æµ‹è¯•ï¼ˆå¯é€‰ï¼‰
    # interactive_test(model, tokenizer)
```

### 5.2 å¯¹æ¯”æµ‹è¯•

åˆ›å»º `scripts/training/compare_models.py`ï¼š

```python
"""
æ¨¡å‹å¯¹æ¯”è„šæœ¬
å¯¹æ¯”åŸå§‹æ¨¡å‹å’Œå¾®è°ƒåæ¨¡å‹çš„æ•ˆæœ
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import json
from typing import Dict, List

BASE_MODEL = "Qwen/Qwen2.5-7B-Instruct"
FINETUNED_PATH = "./trading-llm-lora/final_lora"

def load_base_model():
    """åŠ è½½åŸå§‹æ¨¡å‹"""
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        load_in_4bit=True,
        device_map="auto",
    )
    return model, tokenizer

def load_finetuned_model():
    """åŠ è½½å¾®è°ƒåæ¨¡å‹"""
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL,
        torch_dtype=torch.float16,
        load_in_4bit=True,
        device_map="auto",
    )
    model = PeftModel.from_pretrained(base_model, FINETUNED_PATH)
    return model, tokenizer

def generate(model, tokenizer, prompt: str) -> str:
    """ç”Ÿæˆå“åº”"""
    messages = [{"role": "user", "content": prompt}]
    
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer([text], return_tensors="pt").to(model.device)
    
    outputs = model.generate(
        **inputs,
        max_new_tokens=200,
        temperature=0.1,
        do_sample=False,
    )
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    
    if "assistant" in response:
        return response.split("assistant")[-1].strip()
    return response

def compare():
    """å¯¹æ¯”ä¸¤ä¸ªæ¨¡å‹"""
    
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    base_model, base_tokenizer = load_base_model()
    finetuned_model, finetuned_tokenizer = load_finetuned_model()
    
    # æµ‹è¯•é—®é¢˜
    test_cases = [
        {
            "name": "æ­£å¸¸ä¸Šæ¶¨è¶‹åŠ¿",
            "market_data": {
                "symbol": "BTC/USDT",
                "price": 52000,
                "change_percent": 2.5,
                "technical": {
                    "rsi": 55,
                    "trend_direction": "bullish",
                    "trend_strength": 0.7,
                    "atr_percent": 0.025
                }
            }
        },
        {
            "name": "è¶…å–åå¼¹",
            "market_data": {
                "symbol": "BTC/USDT",
                "price": 48000,
                "change_percent": -3.5,
                "technical": {
                    "rsi": 28,
                    "trend_direction": "bullish",
                    "trend_strength": 0.4,
                    "atr_percent": 0.035
                }
            }
        },
        {
            "name": "é«˜ä½éœ‡è¡",
            "market_data": {
                "symbol": "BTC/USDT",
                "price": 58000,
                "change_percent": 0.5,
                "technical": {
                    "rsi": 68,
                    "trend_direction": "neutral",
                    "trend_strength": 0.3,
                    "atr_percent": 0.02
                }
            }
        }
    ]
    
    print("\n" + "="*70)
    print("ğŸ” æ¨¡å‹å¯¹æ¯”æµ‹è¯•")
    print("="*70)
    
    for case in test_cases:
        print(f"\nğŸ“Œ æµ‹è¯•åœºæ™¯: {case['name']}")
        print("-"*70)
        
        prompt = f"""åˆ†æä»¥ä¸‹å¸‚åœºæ•°æ®ï¼Œç»™å‡ºäº¤æ˜“å»ºè®®ã€‚

ä»·æ ¼: {case['market_data']['price']}
è¶‹åŠ¿: {case['market_data']['technical']['trend_direction']}
å¼ºåº¦: {case['market_data']['technical']['trend_strength']}
RSI: {case['market_data']['technical']['rsi']}"""

        # åŸå§‹æ¨¡å‹
        base_response = generate(base_model, base_tokenizer, prompt)
        print(f"åŸå§‹æ¨¡å‹:\n{base_response[:300]}...")
        print()
        
        # å¾®è°ƒæ¨¡å‹
        finetuned_response = generate(finetuned_model, finetuned_tokenizer, prompt)
        print(f"å¾®è°ƒæ¨¡å‹:\n{finetuned_response[:300]}...")
        print()
    
    print("="*70)
    print("âœ… å¯¹æ¯”æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    compare()
```

---

## é›†æˆåˆ°äº¤æ˜“ç³»ç»Ÿ

### 6.1 åˆ›å»ºæœ¬åœ°æ¨¡å‹æä¾›å•†

åœ¨ `alpha_trading_bot/ai/` ç›®å½•ä¸‹åˆ›å»º `local_provider.py`ï¼š

```python
"""
æœ¬åœ°å¾®è°ƒæ¨¡å‹æä¾›å•†
é›†æˆè®­ç»ƒå¥½çš„ LoRA æ¨¡å‹åˆ°äº¤æ˜“ç³»ç»Ÿ
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
from typing import Dict, Any, Optional, Tuple
import logging
import re

logger = logging.getLogger(__name__)


class LocalLLMProvider:
    """æœ¬åœ°å¾®è°ƒæ¨¡å‹æä¾›å•†"""
    
    def __init__(
        self,
        model_path: str,
        base_model_name: str = "Qwen/Qwen2.5-7B-Instruct",
        device: str = "auto",
        system_prompt: str = None
    ):
        """
        åˆå§‹åŒ–æœ¬åœ°æ¨¡å‹æä¾›å•†
        
        Args:
            model_path: LoRA æ¨¡å‹è·¯å¾„
            base_model_name: åŸºç¡€æ¨¡å‹åç§°
            device: è®¾å¤‡ ("auto", "cuda", "cpu")
            system_prompt: ç³»ç»Ÿæç¤ºè¯
        """
        self.model_path = model_path
        self.base_model_name = base_model_name
        self.system_prompt = system_prompt or self._default_system_prompt()
        
        logger.info(f"[LocalLLM] æ­£åœ¨åŠ è½½æ¨¡å‹: {model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        
        # åŠ è½½æ¨¡å‹
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            load_in_4bit=True,
            device_map=device,
        )
        
        # åŠ è½½ LoRA é€‚é…å™¨
        self.model = PeftModel.from_pretrained(self.model, model_path)
        
        logger.info(f"[LocalLLM] âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _default_system_prompt(self) -> str:
        """é»˜è®¤ç³»ç»Ÿæç¤ºè¯"""
        return """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŠ å¯†è´§å¸äº¤æ˜“åŠ©æ‰‹ã€‚æ ¹æ®æŠ€æœ¯æŒ‡æ ‡å’Œå¸‚åœºæ•°æ®ï¼Œç»™å‡ºç®€æ´æ˜ç¡®çš„äº¤æ˜“å»ºè®®ã€‚

è¯·éµå¾ªä»¥ä¸‹æ ¼å¼å›å¤ï¼š
1. ä¿¡å·ï¼šBUY / SELL / HOLD
2. ç½®ä¿¡åº¦ï¼š0-100%
3. ä¸»è¦ç†ç”±ï¼ˆ1-2å¥è¯ï¼‰
4. é£é™©æç¤ºï¼ˆå¦‚é€‚ç”¨ï¼‰"""
    
    async def get_signal(
        self,
        market_data: Dict[str, Any],
        api_key: str = ""
    ) -> Tuple[str, int]:
        """
        ç”Ÿæˆäº¤æ˜“ä¿¡å·
        
        Args:
            market_data: å¸‚åœºæ•°æ®
            api_key: APIå¯†é’¥ï¼ˆæœ¬åœ°æ¨¡å‹ä¸éœ€è¦ï¼‰
        
        Returns:
            (signal, confidence): ä¿¡å·å’Œç½®ä¿¡åº¦
        """
        try:
            prompt = self._build_prompt(market_data)
            
            # ç”Ÿæˆ
            response = self._generate(prompt)
            
            # è§£æ
            signal = self._parse_signal(response)
            confidence = self._extract_confidence(response)
            
            logger.info(f"[LocalLLM] ä¿¡å·: {signal}, ç½®ä¿¡åº¦: {confidence}%")
            
            return signal, confidence
            
        except Exception as e:
            logger.error(f"[LocalLLM] ç”Ÿæˆä¿¡å·å¤±è´¥: {e}")
            return "HOLD", 50  # é»˜è®¤è¿”å› HOLD
    
    def _build_prompt(self, market_data: Dict[str, Any]) -> str:
        """æ„å»ºæç¤ºè¯"""
        technical = market_data.get("technical", {})
        
        return f"""{self.system_prompt}

å½“å‰å¸‚åœºæ•°æ®ï¼š
- äº¤æ˜“å¯¹ï¼š{market_data.get('symbol', 'BTC/USDT')}
- å½“å‰ä»·æ ¼ï¼š{market_data.get('price', 0)}
- 24hæ¶¨è·Œå¹…ï¼š{market_data.get('change_percent', 0)}%
- RSI(14)ï¼š{technical.get('rsi', 50)}
- MACDï¼š{technical.get('macd_state', 'normal')}
- è¶‹åŠ¿æ–¹å‘ï¼š{technical.get('trend_direction', 'neutral')}
- è¶‹åŠ¿å¼ºåº¦ï¼š{technical.get('trend_strength', 0)}
- ATR%ï¼š{technical.get('atr_percent', 0)}
- ä»·æ ¼å¸ƒæ—å¸¦ä½ç½®ï¼š{technical.get('bb_position', 0)}%"""
    
    def _generate(self, prompt: str) -> str:
        """ç”Ÿæˆå“åº”"""
        messages = [{"role": "user", "content": prompt}]
        
        text = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
        
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=256,
            temperature=0.1,
            top_p=0.9,
            do_sample=False,
        )
        
        response = self.tokenizer.decode(
            outputs[0],
            skip_special_tokens=True
        )
        
        # æå– assistant éƒ¨åˆ†
        if "assistant" in response:
            return response.split("assistant")[-1].strip()
        return response
    
    def _parse_signal(self, response: str) -> str:
        """è§£æä¿¡å·"""
        response_upper = response.upper()
        
        if "BUY" in response_upper and "SELL" not in response_upper:
            return "BUY"
        elif "SELL" in response_upper:
            return "SELL"
        else:
            return "HOLD"
    
    def _extract_confidence(self, response: str) -> int:
        """æå–ç½®ä¿¡åº¦"""
        match = re.search(r'(\d+)%?', response)
        if match:
            return min(100, max(0, int(match.group(1))))
        return 70  # é»˜è®¤ç½®ä¿¡åº¦
    
    def batch_generate(
        self,
        market_data_list: List[Dict[str, Any]]
    ) -> List[Tuple[str, int]]:
        """
        æ‰¹é‡ç”Ÿæˆä¿¡å·
        
        Args:
            market_data_list: å¸‚åœºæ•°æ®åˆ—è¡¨
        
        Returns:
            ä¿¡å·åˆ—è¡¨
        """
        results = []
        
        for market_data in market_data_list:
            signal, confidence = self.get_signal(market_data)
            results.append((signal, confidence))
        
        return results


# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    import asyncio
    
    provider = LocalLLMProvider(
        model_path="./trading-llm-lora/final_lora",
        base_model_name="Qwen/Qwen2.5-7B-Instruct"
    )
    
    market_data = {
        "symbol": "BTC/USDT",
        "price": 50000,
        "change_percent": 2.5,
        "technical": {
            "rsi": 45,
            "trend_direction": "bullish",
            "trend_strength": 0.65,
            "atr_percent": 0.025,
            "bb_position": 50
        }
    }
    
    signal, confidence = asyncio.run(provider.get_signal(market_data))
    print(f"ä¿¡å·: {signal}, ç½®ä¿¡åº¦: {confidence}%")
```

### 6.2 æ›´æ–°é…ç½®

æ›´æ–° `alpha_trading_bot/ai/providers.py`ï¼š

```python
"""
AIæä¾›å•†é…ç½®
"""

PROVIDERS = {
    "deepseek": {
        "base_url": "https://api.deepseek.com/chat/completions",
        "model": "deepseek-chat",
        "type": "remote",
    },
    "kimi": {
        "base_url": "https://api.moonshot.cn/v1/chat/completions",
        "model": "moonshot-v1-8k",
        "type": "remote",
    },
    "openai": {
        "base_url": "https://api.openai.com/v1/chat/completions",
        "model": "gpt-4o-mini",
        "type": "remote",
    },
    "qwen": {
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "model": "qwen-turbo",
        "type": "remote",
    },
    # æœ¬åœ°æ¨¡å‹
    "local_trading": {
        "type": "local",
        "model_path": "./trading-llm-lora/final_lora",
        "base_model": "Qwen/Qwen2.5-7B-Instruct",
        "description": "æœ¬åœ°å¾®è°ƒäº¤æ˜“æ¨¡å‹",
    },
}


def get_provider_config(provider: str) -> dict:
    """è·å–æä¾›å•†é…ç½®"""
    return PROVIDERS.get(provider, PROVIDERS["deepseek"])
```

### 6.3 æ›´æ–° AI å®¢æˆ·ç«¯

æ›´æ–° `alpha_trading_bot/ai/client.py` ä¸­çš„ `_get_single_signal` æ–¹æ³•ï¼š

```python
class AIClient:
    # ... å…¶ä»–ä»£ç  ...
    
    async def _get_single_signal(self, market_data: Dict[str, Any]) -> str:
        """å•AIæ¨¡å¼"""
        provider = self.config.default_provider
        provider_config = get_provider_config(provider)
        
        # åˆ¤æ–­æ˜¯å¦æœ¬åœ°æ¨¡å‹
        if provider_config.get("type") == "local":
            # å¯¼å…¥æœ¬åœ°æ¨¡å‹æä¾›å•†
            from .local_provider import LocalLLMProvider
            
            if not hasattr(self, '_local_provider'):
                self._local_provider = LocalLLMProvider(
                    model_path=provider_config["model_path"],
                    base_model_name=provider_config["base_model"],
                )
            
            signal, confidence = await self._local_provider.get_signal(market_data)
            
            logger.info(f"[AI] æœ¬åœ°æ¨¡å‹: {signal} (ç½®ä¿¡åº¦: {confidence}%)")
            
            return signal
        
        # è¿œç¨‹APIè°ƒç”¨ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
        api_key = self.api_keys.get(provider, "")
        response = await self._call_ai_with_retry(provider, market_data, api_key)
        signal, confidence = parse_response(response)
        
        return signal
```

---

## æŒç»­ä¼˜åŒ–

### 7.1 å¢é‡è®­ç»ƒ

å½“ç§¯ç´¯æ–°æ•°æ®åï¼Œå¯ä»¥å¢é‡è®­ç»ƒï¼š

```python
# scripts/training/incremental_train.py

def incremental_train(
    new_data_path: str,
    base_model_path: str,
    output_path: str,
    learning_rate: float = 5e-5
):
    """
    å¢é‡è®­ç»ƒ
    
    Args:
        new_data_path: æ–°æ•°æ®è·¯å¾„
        base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
        learning_rate: å­¦ä¹ ç‡ï¼ˆæ¯”åˆå§‹è®­ç»ƒå°ï¼‰
    """
    from unsloth import UnslothModel
    from transformers import TrainingArguments, Trainer
    from datasets import load_dataset
    
    # åŠ è½½æ¨¡å‹
    model, tokenizer = UnslothModel.from_pretrained(
        base_model_path,
        max_seq_length=2048,
        dtype=torch.float16,
        load_in_4bit=True,
    )
    
    # åŠ è½½æ–°æ•°æ®
    dataset = load_dataset("json", data_files=new_data_path, split="train")
    
    # æ ¼å¼åŒ–
    def formatting_prompts_func(examples):
        texts = []
        for messages in examples["messages"]:
            text = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
            texts.append(text)
        return {"text": texts}
    
    dataset = dataset.map(
        formatting_prompts_func,
        batched=True,
        remove_columns=["messages"]
    )
    
    # è®­ç»ƒå‚æ•°ï¼ˆæ›´å°çš„å­¦ä¹ ç‡ï¼‰
    training_args = TrainingArguments(
        output_dir=output_path,
        num_train_epochs=2,
        per_device_train_batch_size=2,
        learning_rate=learning_rate,
        fp16=True,
        optim="adamw_8bit",
    )
    
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
    )
    
    trainer.train()
    
    # ä¿å­˜
    model.save_pretrained(output_path)
    tokenizer.save_pretrained(output_path)
    
    print(f"âœ… å¢é‡è®­ç»ƒå®Œæˆ: {output_path}")


if __name__ == "__main__":
    incremental_train(
        new_data_path="training_data/new_data.json",
        base_model_path="./trading-llm-lora/final_lora",
        output_path="./trading-llm-lora/v2",
        learning_rate=5e-5
    )
```

### 7.2 æ¨¡å‹è¯„ä¼°æŒ‡æ ‡

å»ºè®®ç›‘æ§ä»¥ä¸‹æŒ‡æ ‡ï¼š

| æŒ‡æ ‡ | ç›®æ ‡å€¼ | è¯´æ˜ |
|------|--------|------|
| ä¿¡å·å‡†ç¡®ç‡ | > 60% | é¢„æµ‹æ­£ç¡®çš„æ¯”ä¾‹ |
| ç›ˆåˆ©ä¿¡å·æ¯”ä¾‹ | > 55% | BUY/SELL ä¿¡å·çš„ç›ˆåˆ©æ¯”ä¾‹ |
| å¹³å‡ç½®ä¿¡åº¦ | 60-80% | ç½®ä¿¡åº¦ä¸å®œè¿‡ä½æˆ–è¿‡é«˜ |
| å“åº”å»¶è¿Ÿ | < 2ç§’ | å•æ¬¡æ¨ç†æ—¶é—´ |

### 7.3 A/B æµ‹è¯•

```python
# scripts/training/ab_test.py

def ab_test(provider_a: str, provider_b: str, test_data_path: str):
    """
    A/B æµ‹è¯•ä¸¤ä¸ªæ¨¡å‹/æä¾›å•†
    
    Args:
        provider_a: æä¾›å•†Aæ ‡è¯†
        provider_b: æä¾›å•†Bæ ‡è¯†
        test_data_path: æµ‹è¯•æ•°æ®è·¯å¾„
    """
    # å®ç°ä¸¤ä¸ªæ¨¡å‹çš„å¯¹æ¯”æµ‹è¯•
    # è®°å½•å‡†ç¡®ç‡ã€å»¶è¿Ÿã€ç›ˆåˆ©ä¿¡å·æ¯”ä¾‹ç­‰
    pass
```

---

## æˆæœ¬ä¸æ—¶é—´ä¼°ç®—

### ç¡¬ä»¶è¦æ±‚

| é…ç½® | æ˜¾å­˜ | è®­ç»ƒæ—¶é—´ | æˆæœ¬/å°æ—¶ |
|------|------|----------|----------|
| RTX 4090 24GB | 16GB | 2-4å°æ—¶ | $0.5-1 |
| RTX 4080 16GB | 12GB | 3-5å°æ—¶ | $0.4-0.8 |
| A100 40GB | 32GB | 1-2å°æ—¶ | $1-2 |
| Google Colab Pro | 16GB | 4-8å°æ—¶ | $10/æœˆ |

### æ—¶é—´è§„åˆ’

| é˜¶æ®µ | æ—¶é—´ | è¯´æ˜ |
|------|------|------|
| ç¯å¢ƒå‡†å¤‡ | 30åˆ†é’Ÿ | å®‰è£…ä¾èµ– |
| æ•°æ®æ”¶é›† | 2-4å°æ—¶ | æ”¶é›†å’Œæ ‡æ³¨ |
| æ•°æ®æ ¼å¼åŒ– | 1å°æ—¶ | æ ¼å¼è½¬æ¢ |
| è®­ç»ƒ | 2-4å°æ—¶ | LoRAå¾®è°ƒ |
| æµ‹è¯• | 1å°æ—¶ | æ•ˆæœéªŒè¯ |
| é›†æˆ | 1å°æ—¶ | æ¥å…¥ç³»ç»Ÿ |
| **æ€»è®¡** | **8-12å°æ—¶** | - |

### æˆæœ¬ä¼°ç®—

| é¡¹ç›® | è´¹ç”¨ |
|------|------|
| GPUç§Ÿç”¨ï¼ˆRTX 4090ï¼Œ4å°æ—¶ï¼‰ | $2-4 |
| äº‘å­˜å‚¨ | $1-2/æœˆ |
| APIè°ƒç”¨ï¼ˆæµ‹è¯•æ—¶ï¼‰ | $0-5 |
| **æ€»è®¡** | **$5-10** |

---

## å¿«é€Ÿå¯åŠ¨æ¸…å•

- [ ] å®‰è£… Python 3.10+ å’Œ CUDA
- [ ] åˆ›å»º conda/venv è™šæ‹Ÿç¯å¢ƒ
- [ ] å®‰è£… PEFTã€Unslothã€Transformers
- [ ] å‡†å¤‡ 300+ æ¡æ ‡æ³¨æ•°æ®
- [ ] è¿è¡Œ `python scripts/training/prepare_data.py`
- [ ] è¿è¡Œ `python scripts/training/split_data.py`
- [ ] è¿è¡Œ `python scripts/training/train_lora.py`
- [ ] è¿è¡Œ `python scripts/training/test_model.py`
- [ ] é›†æˆåˆ°äº¤æ˜“ç³»ç»Ÿ
- [ ] å°èµ„é‡‘å®ç›˜æµ‹è¯• 1-2 å‘¨

---

## å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ

A: ä¿®æ”¹ä»¥ä¸‹é…ç½®ï¼š
```python
load_in_4bit=True  # 4bité‡åŒ–
MAX_SEQ_LENGTH = 1024  # å‡å°åºåˆ—é•¿åº¦
PER_DEVICE_TRAIN_BATCH_SIZE = 1  # å‡å°batch size
```

### Q2: è®­ç»ƒlossä¸ä¸‹é™æ€ä¹ˆåŠï¼Ÿ

A: æ£€æŸ¥ï¼š
1. æ•°æ®è´¨é‡æ˜¯å¦OK
2. å­¦ä¹ ç‡æ˜¯å¦åˆé€‚ï¼ˆå°è¯• 1e-4 æˆ– 5e-4ï¼‰
3. æ•°æ®æ˜¯å¦æ­£ç¡®æ ¼å¼åŒ–

### Q3: æ¨¡å‹è¾“å‡ºä¸ç¨³å®šæ€ä¹ˆåŠï¼Ÿ

A: æ¨ç†æ—¶ä½¿ç”¨ï¼š
```python
temperature=0.1  # ä½æ¸©åº¦
do_sample=False  # greedyè§£ç 
```

### Q4: å¦‚ä½•æé«˜å‡†ç¡®ç‡ï¼Ÿ

A:
1. å¢åŠ é«˜è´¨é‡è®­ç»ƒæ•°æ®
2. ä½¿ç”¨æ›´å¥½çš„æ ‡æ³¨
3. å°è¯•æ›´å¤§çš„æ¨¡å‹ï¼ˆå¦‚ 14Bï¼‰
4. è°ƒæ•´ LoRA å‚æ•°

---

## å‚è€ƒèµ„æº

- [PEFT æ–‡æ¡£](https://huggingface.co/docs/peft)
- [Unsloth GitHub](https://github.com/unslothai/unsloth)
- [Qwen2.5-Instruct](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- [Llama-3-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)
- [Hugging Face å¾®è°ƒæŒ‡å—](https://huggingface.co/docs/transformers/en/training)

---

> **å…è´£å£°æ˜**: æœ¬æ¨¡å‹ä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚åŠ å¯†è´§å¸äº¤æ˜“å­˜åœ¨é«˜é£é™©ï¼Œè¯·è°¨æ…å†³ç­–ã€‚
